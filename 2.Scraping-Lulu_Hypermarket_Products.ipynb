{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # HTML data structur\n",
    "import requests\n",
    "from urllib.request import urlopen as uReq  # Web client\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV / Excel Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------Creating CSV file for saving scraped data--------------------------#\n",
    "\n",
    "path_1 = \"C:/Users/Mazhar/Projects_Jupyter/5.Scraping-Lulu_Hypermarket/csv_excel_files/Lulu_Products.csv\"\n",
    "csv_file = open(path_1, 'w')\n",
    "csv_writer = csv.writer(csv_file)\n",
    "#defining headers\n",
    "csv_writer.writerow(['Product Titles', 'Brands', 'Category', 'Description', 'Price', 'Image_1', 'Image_2', 'Image_3'])\n",
    "\n",
    "#----------------------Calling Links file------------------------------------------------#\n",
    "\n",
    "path_2 = 'C:/Users/Mazhar/Projects_Jupyter/5.Scraping-Lulu_Hypermarket/csv_excel_files/Lulu_Product_Links.csv'\n",
    "all_products = []\n",
    "none_string = 'NaN'\n",
    "df_linksFile = pd.read_csv(path_2)\n",
    "links = df_linksFile.values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Products Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(0, len(links)+1), desc =\"Progress\"):\n",
    "    try:\n",
    "        # Specify the web page\n",
    "        url = links[i][0]\n",
    "\n",
    "        # Get the web page\n",
    "        response = requests.get(url)\n",
    "        # Data in the web page\n",
    "        data = response.text\n",
    "\n",
    "        # Pass the source code of the web page to beautiful soup to create a beautiful soup object for it.\n",
    "        soup = BeautifulSoup(data, 'html.parser')\n",
    "        #print(soup.prettify())\n",
    "    except Exception as e:\n",
    "        soup = none_string\n",
    "        \n",
    "    try:\n",
    "        product_name = soup.find_all('h1', {'class': 'pdp-productname'})\n",
    "        product_name = product_name[0].text\n",
    "    except Exception as e:\n",
    "        product_name = none_string\n",
    "\n",
    "\n",
    "    try:\n",
    "        brand_name = soup.find_all('strong') #, {'class': 'search-inner-title select-category'})\n",
    "        brand_name = brand_name[1].text.split(\": \")[1]\n",
    "    except Exception as e:\n",
    "        brand_name = none_string\n",
    "\n",
    "    try:\n",
    "        category = soup.find_all('ol')\n",
    "        category = str(category[0].text.split()[7:])\n",
    "        category = category.replace(\"['\",\"\").replace(\"'>'\",\"/\").replace(\"'\",\"\").replace(\"]\",\"\").replace(\",\",\"\").replace(\" / \",\"/\")\n",
    "    except Exception as e:\n",
    "        category = none_string\n",
    "\n",
    "    try:\n",
    "        description = soup.find_all('div', {'class': 'description'})\n",
    "        description = description[0]\n",
    "    except Exception as e:\n",
    "        description = none_string\n",
    "\n",
    "    try:\n",
    "        price = soup.find_all('div', {'class': 'prod-price'})\n",
    "        price = price[0].text[5:-3]\n",
    "    except Exception as e:\n",
    "        price = none_string\n",
    "\n",
    "\n",
    "    #images = soup.find_all('img', attrs = {\"alt\": product_name})\n",
    "    #print(soup.prettify())\n",
    "    #len(images)\n",
    "\n",
    "    try:\n",
    "        images = soup.find_all('img', attrs = {\"alt\": product_name})\n",
    "        image_1 = str(images[0]).split(\"src\")[1].replace('=\"','https://www.luluhypermarket.com')\n",
    "        image_1 = image_1.replace('\"/>','')\n",
    "        image_1\n",
    "    except Exception as e:\n",
    "        image_1 = none_string\n",
    "\n",
    "    try:\n",
    "        images = soup.find_all('img', attrs = {\"alt\": product_name})\n",
    "        image_2 = str(images[1]).split(\"src\")[1].replace('=\"','https://www.luluhypermarket.com')\n",
    "        image_2 = image_2.replace('\"/>','')\n",
    "        image_2\n",
    "    except Exception as e:\n",
    "        image_2 = none_string\n",
    "\n",
    "    try:\n",
    "        images = soup.find_all('img', attrs = {\"alt\": product_name})\n",
    "        image_3 = str(images[2]).split(\"src\")[1].replace('=\"','https://www.luluhypermarket.com')\n",
    "        image_3 = image_3.replace('\"/>','')\n",
    "        image_3\n",
    "    except Exception as e:\n",
    "        image_3 = none_string\n",
    "\n",
    "    # Writing to CSV file\n",
    "    try:\n",
    "        csv_writer.writerow([product_name, brand_name, category, description, price, image_1, image_2, image_3]) \n",
    "    except Exception as e:\n",
    "        csv_writer.writerow([none_string, none_string, none_string, none_string, none_string, none_string, none_string, none_string])\n",
    "    #print(\"\\r Progress\", i, end='')\n",
    "    #time.sleep(0.5)\n",
    "    \n",
    "#closing CSV file\n",
    "csv_file.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file = 'C:/Users/Mazhar/Projects_Jupyter/5.Scraping-Lulu_Hypermarket/csv_excel_files/Lulu_Products.xlsx'\n",
    "df = pd.read_excel(file, index_col=0) \n",
    "df[\"Category\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
